{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mItsCsKF31bT"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, datasets\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mnist_classification():\n",
        "    print(\"\\n--- MNIST Classification ---\")\n",
        "\n",
        "    # Load and preprocess\n",
        "    (x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
        "    x_train, x_test = x_train.reshape(-1, 784) / 255.0, x_test.reshape(-1, 784) / 255.0\n",
        "\n",
        "    # Build and train\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(128, activation=\"relu\", input_shape=(784,)),\n",
        "        layers.Dense(64, activation=\"relu\"),\n",
        "        layers.Dense(10, activation=\"softmax\")\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    model.fit(x_train, y_train, epochs=5, batch_size=32, verbose=1)\n",
        "\n",
        "    # Evaluate\n",
        "    _, acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print(f\"Test Accuracy: {acc:.4f}\")\n",
        "\n",
        "# IMDB Classification\n",
        "def imdb_classification():\n",
        "    print(\"\\n--- IMDB Sentiment Classification ---\")\n",
        "\n",
        "    # Load and preprocess\n",
        "    (x_train, y_train), (x_test, y_test) = datasets.imdb.load_data(num_words=10000)\n",
        "    x_train, x_test = pad_sequences(x_train, maxlen=256), pad_sequences(x_test, maxlen=256)\n",
        "\n",
        "    # Build and train\n",
        "    model = models.Sequential([\n",
        "        layers.Embedding(10000, 16),\n",
        "        layers.GlobalAveragePooling1D(),\n",
        "        layers.Dense(16, activation=\"relu\"),\n",
        "        layers.Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "    model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "    # Evaluate\n",
        "    _, acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print(f\"Test Accuracy: {acc:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mnist_classification()\n",
        "    imdb_classification()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L82_FVkc4EC_",
        "outputId": "cf972533-6931-43ce-a2ab-5e435244b926"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- MNIST Classification ---\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.8762 - loss: 0.4335\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9663 - loss: 0.1094\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9788 - loss: 0.0690\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9848 - loss: 0.0497\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9877 - loss: 0.0378\n",
            "Test Accuracy: 0.9751\n",
            "\n",
            "--- IMDB Sentiment Classification ---\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Epoch 1/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.6229 - loss: 0.6529 - val_accuracy: 0.8058 - val_loss: 0.4205\n",
            "Epoch 2/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8612 - loss: 0.3505 - val_accuracy: 0.8696 - val_loss: 0.3230\n",
            "Epoch 3/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8940 - loss: 0.2660 - val_accuracy: 0.8840 - val_loss: 0.2887\n",
            "Epoch 4/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9177 - loss: 0.2196 - val_accuracy: 0.8866 - val_loss: 0.2843\n",
            "Epoch 5/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9266 - loss: 0.1916 - val_accuracy: 0.8874 - val_loss: 0.2831\n",
            "Epoch 6/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9349 - loss: 0.1719 - val_accuracy: 0.8896 - val_loss: 0.2898\n",
            "Epoch 7/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.9481 - loss: 0.1504 - val_accuracy: 0.8854 - val_loss: 0.3022\n",
            "Epoch 8/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9488 - loss: 0.1382 - val_accuracy: 0.8724 - val_loss: 0.3433\n",
            "Epoch 9/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9572 - loss: 0.1251 - val_accuracy: 0.8860 - val_loss: 0.3261\n",
            "Epoch 10/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9632 - loss: 0.1122 - val_accuracy: 0.8794 - val_loss: 0.3477\n",
            "Test Accuracy: 0.8692\n"
          ]
        }
      ]
    }
  ]
}