{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_t90861D1xAe",
        "outputId": "5ec482e3-fea8-4c38-81c7-b8dcfd718a5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated: project guteteerrrrraraaaaaaa          te\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "raw_text = \"project gutenberg australia news and views from the australian chapter of the project gutenberg literary archive foundation\"\n",
        "corpus = raw_text.lower().replace('\\n', ' ')\n",
        "\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts([corpus])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "tokens = tokenizer.texts_to_sequences([corpus])[0]\n",
        "input_sequences = [tokens[:i+1] for i in range(1, len(tokens))]\n",
        "max_len = max(len(x) for x in input_sequences)\n",
        "\n",
        "\n",
        "padded = pad_sequences(input_sequences, maxlen=max_len, padding='pre')\n",
        "X, y = padded[:, :-1], tf.keras.utils.to_categorical(padded[:, -1], vocab_size)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, 10, input_length=max_len-1),\n",
        "    tf.keras.layers.LSTM(100),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=50, verbose=0) # Increased epochs slightly for better results\n",
        "\n",
        "def generate(seed, chars):\n",
        "    result = seed\n",
        "    for _ in range(chars):\n",
        "\n",
        "        token_list = pad_sequences(tokenizer.texts_to_sequences([result])[0:1], maxlen=max_len-1, padding='pre')\n",
        "        pred_idx = np.argmax(model.predict(token_list, verbose=0), axis=-1)[0]\n",
        "\n",
        "\n",
        "        if pred_idx != 0:\n",
        "            result += tokenizer.index_word[pred_idx]\n",
        "    return result\n",
        "\n",
        "print(f\"Generated: {generate('project gut', 30)}\")"
      ]
    }
  ]
}