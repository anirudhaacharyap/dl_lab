{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQ76UdqgR51X",
        "outputId": "08c51b6c-792b-4b70-9213-7083e64ee535"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with 8 hidden units\n",
            "Final loss: 0.8464526534080505\n",
            "\n",
            "Training with 16 hidden units\n",
            "Final loss: 0.12174487113952637\n",
            "\n",
            "Best Model Loss: 0.12174487113952637\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step\n",
            "\n",
            "True Output:\n",
            "[[0.77576443 0.81216552]\n",
            " [1.20014134 0.1393125 ]\n",
            " [1.34733826 0.19964421]\n",
            " [1.42501386 0.45633508]\n",
            " [0.7088419  0.37942484]]\n",
            "\n",
            "Predicted Output:\n",
            "[[1.0008904  0.6019724 ]\n",
            " [0.8581322  0.3453722 ]\n",
            " [1.2246304  0.6187013 ]\n",
            " [1.068818   0.5171817 ]\n",
            " [0.78789306 0.3887134 ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# -------- Generate Simple Data --------\n",
        "X1 = np.random.rand(200, 2)   # Input 1\n",
        "X2 = np.random.rand(200, 1)   # Input 2\n",
        "\n",
        "y1 = X1[:, 0] + X2[:, 0]      # Output 1\n",
        "y2 = X1[:, 1]                 # Output 2\n",
        "\n",
        "# -------- Model Builder --------\n",
        "def build_model(units):\n",
        "    i1 = keras.Input(shape=(2,))\n",
        "    i2 = keras.Input(shape=(1,))\n",
        "\n",
        "    x1 = layers.Dense(units, activation='relu')(i1)\n",
        "    x2 = layers.Dense(units, activation='relu')(i2)\n",
        "\n",
        "    x = layers.concatenate([x1, x2])\n",
        "\n",
        "    o1 = layers.Dense(1)(x)\n",
        "    o2 = layers.Dense(1)(x)\n",
        "\n",
        "    model = keras.Model([i1, i2], [o1, o2])\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "# -------- Hyper-Parameter Tuning --------\n",
        "best_loss = float(\"inf\")\n",
        "\n",
        "for units in [8, 16]:\n",
        "    print(f\"\\nTraining with {units} hidden units\")\n",
        "    model = build_model(units)\n",
        "    history = model.fit([X1, X2], [y1, y2],\n",
        "                        epochs=10, batch_size=16, verbose=0)\n",
        "    loss = history.history['loss'][-1]\n",
        "    print(\"Final loss:\", loss)\n",
        "\n",
        "    if loss < best_loss:\n",
        "        best_loss = loss\n",
        "        best_model = model\n",
        "\n",
        "print(\"\\nBest Model Loss:\", best_loss)\n",
        "\n",
        "# -------- Test Prediction --------\n",
        "pred = best_model.predict([X1[:5], X2[:5]])\n",
        "\n",
        "print(\"\\nTrue Output:\")\n",
        "print(np.column_stack((y1[:5], y2[:5])))\n",
        "\n",
        "print(\"\\nPredicted Output:\")\n",
        "print(np.column_stack(pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdFpxTVZSS61"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "01258679",
        "outputId": "5f0b32ce-efb2-43eb-b29b-8e91f27f0d97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikeras==0.13.0\n",
            "  Using cached scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting scikit-learn==1.4.2\n",
            "  Downloading scikit_learn-1.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting numpy==1.26.0\n",
            "  Downloading numpy-1.26.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from scikeras==0.13.0) (3.10.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.4.2) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.4.2) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.4.2) (3.6.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras==0.13.0) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras==0.13.0) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras==0.13.0) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras==0.13.0) (3.15.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras==0.13.0) (0.17.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras==0.13.0) (0.5.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras==0.13.0) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from optree->keras>=3.2.0->scikeras==0.13.0) (4.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from packaging->keras>=3.2.0->scikeras==0.13.0) (3.2.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.2.0->scikeras==0.13.0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.2.0->scikeras==0.13.0) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras==0.13.0) (0.1.2)\n",
            "Using cached scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
            "Downloading scikit_learn-1.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m118.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.9/17.9 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scikit-learn, scikeras\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.3.0\n",
            "    Uninstalling scikit-learn-1.3.0:\n",
            "      Successfully uninstalled scikit-learn-1.3.0\n",
            "  Attempting uninstall: scikeras\n",
            "    Found existing installation: scikeras 0.4.1\n",
            "    Uninstalling scikeras-0.4.1:\n",
            "      Successfully uninstalled scikeras-0.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray 2025.10.1 requires packaging>=24.1, but you have packaging 21.3 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.0 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.0 which is incompatible.\n",
            "astropy 7.1.1 requires packaging>=22.0.0, but you have packaging 21.3 which is incompatible.\n",
            "db-dtypes 1.4.3 requires packaging>=24.2.0, but you have packaging 21.3 which is incompatible.\n",
            "cuml-cu12 25.6.0 requires scikit-learn>=1.5, but you have scikit-learn 1.4.2 which is incompatible.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.4.2 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.0 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.0 which is incompatible.\n",
            "pandas-gbq 0.30.0 requires packaging>=22.0.0, but you have packaging 21.3 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.0 which is incompatible.\n",
            "libpysal 4.13.0 requires packaging>=22, but you have packaging 21.3 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.0 scikeras-0.13.0 scikit-learn-1.4.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "a81c9104b0af4afcbf640b0c949a6610",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pip install scikeras==0.13.0 scikit-learn==1.4.2 numpy==1.26.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "GU2NeAbzSU8Q",
        "outputId": "7fc6fbc6-910a-4499-db1a-39512d2065c8"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Found input variables with inconsistent numbers of samples: [2, 800]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-980025932.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m )\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX1_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2_train_scaled\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best hyperparameters: {grid_search.best_params_}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0mscorers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefit_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_scorers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_method_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2, 800]"
          ]
        }
      ],
      "source": [
        "#MIMO\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow import keras\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "\n",
        "def generate_mimo_data(n_samples=1000, n_feat1=5, n_feat2=3):\n",
        "    np.random.seed(42)\n",
        "    X1 = np.random.rand(n_samples, n_feat1)\n",
        "    X2 = np.random.rand(n_samples, n_feat2)\n",
        "    y1 = 2 * X1[:, 0] + 3 * X2[:, 1] + np.random.randn(n_samples) * 0.1\n",
        "    y2 = 0.5 * X1[:, 2] - X2[:, 0] + np.random.randn(n_samples) * 0.1\n",
        "    return X1, X2, np.column_stack((y1, y2))\n",
        "\n",
        "X1, X2, Y = generate_mimo_data()\n",
        "X1_train, X1_test, X2_train, X2_test, Y_train, Y_test = train_test_split(\n",
        "    X1, X2, Y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "scaler_X1, scaler_X2 = StandardScaler(), StandardScaler()\n",
        "X1_train_scaled = scaler_X1.fit_transform(X1_train)\n",
        "X1_test_scaled = scaler_X1.transform(X1_test)\n",
        "X2_train_scaled = scaler_X2.fit_transform(X2_train)\n",
        "X2_test_scaled = scaler_X2.transform(X2_test)\n",
        "\n",
        "def create_mimo_model(optimizer='adam', learning_rate=0.01, hidden_units=32):\n",
        "    input1 = keras.layers.Input(shape=(X1.shape[1],), name='input_1')\n",
        "    input2 = keras.layers.Input(shape=(X2.shape[1],), name='input_2')\n",
        "    x1 = keras.layers.Dense(hidden_units, activation='relu')(input1)\n",
        "    x1 = keras.layers.Dense(hidden_units // 2, activation='relu')(x1)\n",
        "    x2 = keras.layers.Dense(hidden_units, activation='relu')(input2)\n",
        "    x2 = keras.layers.Dense(hidden_units // 2, activation='relu')(x2)\n",
        "    merged = keras.layers.concatenate([x1, x2])\n",
        "    output1 = keras.layers.Dense(1, name='output_1')(merged)\n",
        "    output2 = keras.layers.Dense(1, name='output_2')(merged)\n",
        "    model = keras.Model(inputs=[input1, input2], outputs=[output1, output2])\n",
        "    opt = keras.optimizers.Adam(learning_rate=learning_rate) if optimizer == 'adam' else keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=opt, loss={'output_1': 'mse', 'output_2': 'mse'}, metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "keras_regressor = KerasRegressor(model=create_mimo_model, verbose=0)\n",
        "\n",
        "param_grid = {\n",
        "    'model__optimizer': ['adam', 'sgd'],\n",
        "    'model__learning_rate': [0.001, 0.01],\n",
        "    'model__hidden_units': [16, 32, 64],\n",
        "    'epochs': [10, 20],\n",
        "    'batch_size': [16, 32]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=keras_regressor,\n",
        "    param_grid=param_grid,\n",
        "    cv=3,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit([X1_train_scaled, X2_train_scaled], Y_train)\n",
        "\n",
        "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
        "print(f\"Best CV score (neg MSE): {grid_search.best_score_:.4f}\")\n",
        "\n",
        "best_model = grid_search.best_estimator_.model\n",
        "loss, mae1, mae2 = best_model.evaluate(\n",
        "    [X1_test_scaled, X2_test_scaled], Y_test, verbose=0\n",
        ")\n",
        "\n",
        "print(f\"\\nTest Set Performance:\")\n",
        "print(f\"  Loss: {loss:.4f}\")\n",
        "print(f\"  MAE Output 1: {mae1:.4f}\")\n",
        "print(f\"  MAE Output 2: {mae2:.4f}\")\n",
        "\n",
        "preds = best_model.predict([X1_test_scaled[:5], X2_test_scaled[:5]], verbose=0)\n",
        "print(\"\\nSample Predictions (first 5):\")\n",
        "print(f\"  True:      {Y_test[:5]}\")\n",
        "print(f\"  Predicted: {np.array(preds).T}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdBr-oEqS-7L",
        "outputId": "c292ecde-b989-4264-b31d-ce2a206b8921"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n"
          ]
        }
      ],
      "source": [
        "pip install scikeras==0.13.0 scikit-learn==1.4.2 numpy==1.26.0\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow import keras\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "\n",
        "def generate_mimo_data(n_samples=1000, n_feat1=5, n_feat2=3):\n",
        "    np.random.seed(42)\n",
        "    X1 = np.random.rand(n_samples, n_feat1)\n",
        "    X2 = np.random.rand(n_samples, n_feat2)\n",
        "    y1 = 2 * X1[:, 0] + 3 * X2[:, 1] + np.random.randn(n_samples) * 0.1\n",
        "    y2 = 0.5 * X1[:, 2] - X2[:, 0] + np.random.randn(n_samples) * 0.1\n",
        "    return X1, X2, np.column_stack((y1, y2))\n",
        "\n",
        "X1, X2, Y = generate_mimo_data()\n",
        "X1_train, X1_test, X2_train, X2_test, Y_train, Y_test = train_test_split(\n",
        "    X1, X2, Y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "scaler_X1, scaler_X2 = StandardScaler(), StandardScaler()\n",
        "X1_train_scaled = scaler_X1.fit_transform(X1_train)\n",
        "X1_test_scaled = scaler_X1.transform(X1_test)\n",
        "X2_train_scaled = scaler_X2.fit_transform(X2_train)\n",
        "X2_test_scaled = scaler_X2.transform(X2_test)\n",
        "\n",
        "X_train_combined = np.hstack((X1_train_scaled, X2_train_scaled))\n",
        "X_test_combined = np.hstack((X1_test_scaled, X2_test_scaled))\n",
        "\n",
        "def create_mimo_model(optimizer='adam', learning_rate=0.01, hidden_units=32, input_dim=None):\n",
        "    if input_dim is None:\n",
        "        input_dim = X_train_combined.shape[1]\n",
        "    inp = keras.layers.Input(shape=(input_dim,), name='input_combined')\n",
        "    x = keras.layers.Dense(hidden_units, activation='relu')(inp)\n",
        "    x = keras.layers.Dense(hidden_units // 2, activation='relu')(x)\n",
        "    # single output with 2 units (for the two targets) — makes scikeras/GridSearch compatible\n",
        "    outputs = keras.layers.Dense(2, activation='linear', name='outputs')(x)\n",
        "    model = keras.Model(inputs=inp, outputs=outputs)\n",
        "    opt = keras.optimizers.Adam(learning_rate=learning_rate) if optimizer == 'adam' else keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=opt, loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "keras_regressor = KerasRegressor(model=create_mimo_model, verbose=0)\n",
        "\n",
        "param_grid = {\n",
        "    'model__optimizer': ['adam', 'sgd'],\n",
        "    'model__learning_rate': [0.001, 0.01],\n",
        "    'model__hidden_units': [16, 32, 64],\n",
        "    'epochs': [10, 20],\n",
        "    'batch_size': [16, 32]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=keras_regressor,\n",
        "    param_grid=param_grid,\n",
        "    cv=3,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_combined, Y_train)\n",
        "\n",
        "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
        "print(f\"Best CV score (neg MSE): {grid_search.best_score_:.4f}\")\n",
        "\n",
        "best_model = grid_search.best_estimator_.model_  # note the trailing underscore\n",
        "eval_results = best_model.evaluate(X_test_combined, Y_test, verbose=0)\n",
        "print(f\"\\nRaw evaluate() output: {eval_results}\")\n",
        "\n",
        "# evaluate() returns [loss, mae] for this single-output (2-unit) configuration\n",
        "loss = eval_results[0]\n",
        "mae = eval_results[1]\n",
        "\n",
        "print(f\"\\nTest Set Performance:\")\n",
        "print(f\"  Loss: {loss:.4f}\")\n",
        "print(f\"  MAE (averaged over outputs): {mae:.4f}\")\n",
        "\n",
        "preds = best_model.predict(X_test_combined[:5], verbose=0)\n",
        "print(\"\\nSample Predictions (first 5):\")\n",
        "print(f\"  True:      {Y_test[:5]}\")\n",
        "print(f\"  Predicted: {preds}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}